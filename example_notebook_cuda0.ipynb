{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import pickle\n",
    "import ml_collections\n",
    "import wandb, signatory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from tqdm import tqdm\n",
    "sns.set_style(\"darkgrid\")  # 원하는 스타일 선택\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import *\n",
    "from src.evaluation.summary import full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration dict\n",
    "config_dir = 'configs/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "\n",
    "if (config.device == \"cuda\" and torch.cuda.is_available()):\n",
    "    config.update({\"device\": \"cuda:0\"}, allow_val_change=True)\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    config.update({\"device\": \"cpu\"}, allow_val_change=True)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(data, window_size):\n",
    "    n_windows = data.shape[0] - window_size + 1\n",
    "    windows = np.zeros((n_windows, window_size, data.shape[1]))\n",
    "    for idx in range(n_windows):\n",
    "        windows[idx] = data[idx:idx + window_size]\n",
    "    return windows\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "df = pd.read_csv(\"./data/indices.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df.set_index('Date', inplace=True)\n",
    "df = df.apply(pd.to_numeric).astype(float)\n",
    "\n",
    "# Step 2: Compute log returns\n",
    "log_returns = np.diff(np.log(df), axis=0)\n",
    "print(log_returns.shape)\n",
    "\n",
    "# Step 3: Scale the log returns\n",
    "log_returns_scaled, scalers = scaling(log_returns)\n",
    "\n",
    "# Step 4: Prepare initial prices and create rolling windows\n",
    "init_price = torch.from_numpy(np.array(df)[:-(config.n_steps), :]).float().unsqueeze(1)\n",
    "log_returns_scaled = torch.from_numpy(rolling_window(log_returns_scaled, config.n_steps)).float()\n",
    "print('init_price:', init_price.shape)\n",
    "print('log_returns_scaled:', log_returns_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Return Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute cumulative log returns\n",
    "cumulative_log_returns = np.cumsum(log_returns, axis=0)\n",
    "\n",
    "# Step 4: Create a DataFrame for cumulative log returns\n",
    "cumulative_log_returns_df = pd.DataFrame(\n",
    "    cumulative_log_returns,\n",
    "    index=df.index[1:],  # Adjust index for cumulative log returns\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "# Step 5: Plot the cumulative log return paths\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cumulative_log_returns_df.columns:\n",
    "    plt.plot(cumulative_log_returns_df.index, cumulative_log_returns_df[col], label=col)\n",
    "\n",
    "plt.title(\"Cumulative Log Return Paths of 6 Assets\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative Log Returns\", fontsize=14)\n",
    "plt.legend(title=\"Assets\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models for time series generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into training and validation set for the offline evaluation of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_idx = torch.randperm(log_returns_scaled.shape[0])\n",
    "train_size = int(0.7*log_returns_scaled.shape[0])\n",
    "\n",
    "training_data = log_returns_scaled[perm_idx[:train_size]]\n",
    "train_init_price = init_price[perm_idx[:train_size]]\n",
    "test_data = log_returns_scaled[perm_idx[train_size:]]\n",
    "test_init_price = init_price[perm_idx[train_size:]]\n",
    "\n",
    "print(\"training_data: \", training_data.shape)\n",
    "print(\"test_data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = TensorDataset(training_data)\n",
    "test_set = TensorDataset(test_data)\n",
    "\n",
    "train_dl = DataLoader(training_set, batch_size=config.batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a generator and a discriminator for this task. Both the generator and discriminator takes as input the time series. Then we have the training algorithm TailGANTrainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the generator, discriminator and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baselines.networks.discriminators import UserDiscriminator\n",
    "from src.baselines.networks.generators import UserGenerator\n",
    "from src.baselines.trainer import *\n",
    "\n",
    "generator = UserGenerator(config)\n",
    "discriminator = UserDiscriminator(config)\n",
    "trainer = GANTrainer(G=generator, D=discriminator, train_dl=train_dl, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVFIT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, wasserstein_distance, ks_2samp, spearmanr, kendalltau\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "full_name = \"200_256_Glr_0.0001_Dlr_0.0001_Tlr_0.0005_n_heads_2_n_layers_3_batch_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Find the best epochs based on 100 days cumulative distribution \"\"\"\n",
    "window = 100\n",
    "gen_tmp = UserGenerator(config)\n",
    "min_dist = float('inf')  \n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "def compute_avg_emd(real_data, fake_data, window):\n",
    "    \"\"\"\n",
    "    Compute the Earth Mover's Distance (EMD) for real and fake data over a rolling window.\n",
    "    \"\"\"\n",
    "    emd = 0\n",
    "    for i in range(real_data.shape[1]):  # Iterate over features\n",
    "        real_dist = rolling_window(real_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        fake_dist = rolling_window(fake_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        emd += wasserstein_distance(real_dist, fake_dist)\n",
    "    return emd\n",
    "\n",
    "for epoch in range(150, 201, 5):\n",
    "    \n",
    "    # Load generator for the current epoch\n",
    "    generator.load_state_dict(torch.load(f'./results/models/{full_name}/Generator_{epoch}.pt'))\n",
    "    generator.to(config.device)\n",
    "    generator.eval()\n",
    "    \n",
    "    # Generate fake data\n",
    "    fake = []\n",
    "    with torch.no_grad():\n",
    "        for real_batch in train_dl:\n",
    "            real_batch = real_batch[0].transpose(1, 2).to(config.device)\n",
    "            fake_tmp, _= generator(real_batch.shape[0], config.n_steps, config.device, real_batch)        \n",
    "            fake.append(fake_tmp.cpu().detach())            \n",
    "    fake = torch.cat(fake, dim=0)\n",
    "    \n",
    "    # Inverse scaling for real and fake data\n",
    "    fake_data = inverse_scaling(fake, scalers)        \n",
    "    real_data = inverse_scaling(training_data.transpose(1, 2), scalers)\n",
    "    \n",
    "    # Compute EMD\n",
    "    emd = compute_avg_emd(real_data, fake_data, window)\n",
    "\n",
    "    # Update best epoch if current EMD is lower\n",
    "    if emd < min_dist:\n",
    "        min_dist = emd\n",
    "        best_epoch = epoch\n",
    "        print(f\"min_distance: {min_dist:.3f}, best_epoch: {best_epoch}\") \n",
    "\n",
    "generator.load_state_dict(torch.load(f'./results/models/{full_name}/Generator_{best_epoch}.pt'))\n",
    "generator.to(config.device)\n",
    "generator.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fake data와 Test data를 비교 \"\"\"\n",
    "fake = []\n",
    "with torch.no_grad():\n",
    "    for real_batch in train_dl:\n",
    "        real_batch = real_batch[0].transpose(1, 2).to(config.device)        \n",
    "        fake_tmp, _ = generator(real_batch.shape[0], config.n_steps, config.device, real_batch)                        \n",
    "        fake.append(fake_tmp.cpu().detach())        \n",
    "fake = torch.cat(fake, dim=0)\n",
    "\n",
    "fake_data = inverse_scaling(fake, scalers)\n",
    "real_data = inverse_scaling(test_data.transpose(1, 2), scalers)\n",
    "\n",
    "# 각 feature의 전체 최소값 및 최대값 계산\n",
    "min_vals = real_data.min(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "max_vals = real_data.max(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "\n",
    "# 마스크 계산 (모든 샘플이 범위 내에 있는지 확인)\n",
    "mask = np.all((fake_data >= min_vals * 1.5) & (fake_data <= max_vals * 1.5), axis=(1, 2))\n",
    "\n",
    "# 마스크를 적용하여 fake_data 필터링\n",
    "print(real_data.shape, type(real_data))\n",
    "print(fake_data.shape, type(fake_data))\n",
    "fake_data = fake_data[mask]\n",
    "print(f\"Filtered shape: {fake_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" condition은 동일하게 주고 noise만 달리했을 때 결과 \"\"\"\n",
    "fake_new = []\n",
    "with torch.no_grad():\n",
    "    for real_batch in train_dl:\n",
    "        real_batch = real_batch[0].transpose(1, 2).to(config.device)\n",
    "        real_first = real_batch[0].unsqueeze(0)  # Shape: (1, 5, 120)\n",
    "        real_copied = real_first.repeat(real_batch.shape[0], 1, 1)  # Shape: (batch_size, 5, 120)\n",
    "        fake_tmp, _ = generator(real_batch.shape[0], config.n_steps, config.device, real_copied)\n",
    "        fake_new.append(fake_tmp.cpu().detach())\n",
    "fake_data_new = torch.cat(fake_new, dim=0)\n",
    "\n",
    "fake_data_new = inverse_scaling(fake_data_new, scalers)\n",
    "\n",
    "cumulative_log_returns = np.cumsum(fake_data_new[:100, 0:1, :].squeeze(), axis=1)\n",
    "\n",
    "# 각 경로의 시작점을 0으로 설정\n",
    "cumulative_log_returns = np.hstack([np.zeros((cumulative_log_returns.shape[0], 1)), cumulative_log_returns])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):  # 각 샘플에 대해 플롯\n",
    "    plt.plot(cumulative_log_returns[i], alpha=0.6)\n",
    "\n",
    "plt.title('Cumulative Log Returns for All Samples (Starting at 0)')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Log Return')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize the distribution of the real and fake data \"\"\"\n",
    "fake_list = [fake_data[:, i, :] for i in range(fake_data.shape[1])]\n",
    "real_list = [real_data[:, i, :] for i in range(real_data.shape[1])]\n",
    "\n",
    "# Plot the distribution of the real and fake data\n",
    "windows = [1, 5, 20, 100]\n",
    "for j in range(config.n_vars):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(28, 6))  \n",
    "\n",
    "    for i in range(len(windows)):\n",
    "        col = i\n",
    "\n",
    "        real_dist = rolling_window(real_list[j].T, windows[i]).sum(axis=1).ravel()\n",
    "        fake_dist = rolling_window(fake_list[j].T, windows[i]).sum(axis=1).ravel()        \n",
    "        \n",
    "        min_val = real_dist.min()\n",
    "        max_val = real_dist.max()\n",
    "        \n",
    "        bins = np.linspace(min_val, max_val, 81)  \n",
    "        \n",
    "        sns.histplot(real_dist, bins=bins, kde=False, ax=axs[col], color='tab:blue', alpha=0.5, stat='density')\n",
    "        sns.histplot(fake_dist, bins=bins, kde=False, ax=axs[col], color='tab:orange', alpha=0.5, stat='density')\n",
    "\n",
    "        axs[col].set_xlim(*np.quantile(real_dist, [0.001, .999]))\n",
    "        \n",
    "        axs[col].set_title('{} day return distribution'.format(windows[i]), size=18)\n",
    "        axs[col].yaxis.grid(True, alpha=0.5)\n",
    "        axs[col].set_xlabel('Cumulative log return', fontsize=12)\n",
    "        axs[col].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    axs[0].legend(['Historical returns', 'Synthetic returns'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def calculate_distribution_scores(real, fake, num_G, windows):\n",
    "    scores = {\n",
    "        'EMD': np.zeros((num_G, len(windows))),\n",
    "        'KL': np.zeros((num_G, len(windows))),\n",
    "        'JS': np.zeros((num_G, len(windows))),\n",
    "        'KS': np.zeros((num_G, len(windows)))\n",
    "    }\n",
    "\n",
    "    for i in range(num_G):\n",
    "        for j in range(len(windows)):\n",
    "            real_dist = rolling_window(real[i].T, windows[j]).sum(axis=1).ravel()\n",
    "            fake_dist = rolling_window(fake[i].T, windows[j]).sum(axis=1).ravel()\n",
    "            \n",
    "            np.random.shuffle(real_dist)\n",
    "            np.random.shuffle(fake_dist)\n",
    "            \n",
    "            # Calculate EMD\n",
    "            scores['EMD'][i, j] = wasserstein_distance(real_dist, fake_dist)\n",
    "            \n",
    "            # Calculate KS Statistic\n",
    "            scores['KS'][i, j], _ = ks_2samp(real_dist, fake_dist)                                    \n",
    "            \n",
    "            # Create histograms to estimate the probability distributions\n",
    "            real_hist, bin_edges = np.histogram(real_dist, bins=100, density=True)\n",
    "            fake_hist, _ = np.histogram(fake_dist, bins=bin_edges, density=True)\n",
    "            \n",
    "            # Normalize the histograms to get probability distributions\n",
    "            real_prob = real_hist / np.sum(real_hist)\n",
    "            fake_prob = fake_hist / np.sum(fake_hist)\n",
    "            \n",
    "            # Calculate KL Divergence\n",
    "            kl_divergence = entropy(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "            scores['KL'][i, j] = kl_divergence\n",
    "\n",
    "            # Calculate JS Divergence\n",
    "            js_divergence = jensenshannon(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "            scores['JS'][i, j] = js_divergence\n",
    "                \n",
    "    df_scores = {}\n",
    "    for metric, data in scores.items():\n",
    "        data = np.round(data, decimals=4)\n",
    "        df_scores[metric] = pd.DataFrame(data.T, index=windows, columns=[f'{metric} {i}' for i in range(num_G)])\n",
    "        \n",
    "    emd_avg = np.mean(scores['EMD'], axis=0)\n",
    "    df_scores['EMD']['EMD_avg'] = np.round(emd_avg, decimals=4)\n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "# Calculate the distribution scores\n",
    "windows = pd.Series([1, 5, 20, 100], name='window size')\n",
    "\n",
    "results_emd = calculate_distribution_scores(real_list, fake_list, config.n_vars, windows)\n",
    "results_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" visualizing acf plots \"\"\"\n",
    "def plot_acf_comparison(real_list, fake_list, num_G, lags=20):    \n",
    "    data_types = ['Identity', 'Squared']\n",
    "    data_transforms = [lambda x: x, np.square]  \n",
    "    titles = ['Identity log returns', 'Squared log returns']\n",
    "\n",
    "    for i in range(num_G):\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))  \n",
    "        print(i)\n",
    "        print(real_list[i][:, 0])\n",
    "        for ax, data_type, transform, title in zip(axs, data_types, data_transforms, titles):\n",
    "            \n",
    "            transformed_real = transform(real_list[i])\n",
    "            transformed_fake = transform(fake_list[i])\n",
    "            \n",
    "            acf_real = np.array([acf(ts, nlags=lags) for ts in transformed_real])\n",
    "            acf_fake = np.array([acf(ts, nlags=lags) for ts in transformed_fake])\n",
    "            \n",
    "            mean_real = np.mean(acf_real, axis=0)\n",
    "            std_real = np.std(acf_real, axis=0)\n",
    "            mean_fake = np.mean(acf_fake, axis=0)\n",
    "            std_fake = np.std(acf_fake, axis=0)\n",
    "                        \n",
    "            ax.plot(mean_real, label=f'{data_type} ACF Real - Mean', color='tab:blue')\n",
    "            ax.fill_between(range(lags+1), mean_real - 0.5*std_real, mean_real + 0.5*std_real, color='tab:blue', alpha=0.2,\n",
    "                            label=f'{data_type} ACF Real - 1/2 Std Dev')        \n",
    "            ax.plot(mean_fake, label=f'{data_type} ACF Synthetic - Mean', color='tab:orange')\n",
    "            ax.fill_between(range(lags+1), mean_fake - 0.5*std_fake, mean_fake + 0.5*std_fake, color='tab:orange', alpha=0.2,\n",
    "                            label=f'{data_type} ACF Synthetic - 1/2 Std Dev')\n",
    "            \n",
    "            ax.set_ylim(-0.20, 0.3)\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True)\n",
    "            ax.axhline(y=0, color='k')\n",
    "            ax.axvline(x=0, color='k')\n",
    "            ax.set_xlabel('Lag (number of days)', fontsize=16)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "plot_acf_comparison(real_list, fake_list, config.n_vars, lags=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean correlation across all samples\n",
    "def calculate_mean_feature_correlations(data):\n",
    "    num_samples = data.shape[0]\n",
    "    correlations = []\n",
    "\n",
    "    # 각 샘플에 대해 상관관계를 계산하고 결과를 저장\n",
    "    for i in range(num_samples):\n",
    "        sample = data[i, :, :]\n",
    "        correlation_matrix = np.corrcoef(sample, rowvar=False)  # Correlation between features\n",
    "        correlations.append(correlation_matrix)\n",
    "\n",
    "    # 저장된 상관행렬들의 평균을 계산\n",
    "    mean_correlation_matrix = np.mean(correlations, axis=0)\n",
    "    return mean_correlation_matrix\n",
    "\n",
    "# Calculate the mean correlations for both datasets\n",
    "fake_mean_correlation = calculate_mean_feature_correlations(np.transpose(fake_data, (0, 2, 1)))\n",
    "real_mean_correlation = calculate_mean_feature_correlations(np.transpose(real_data, (0, 2, 1)))\n",
    "\n",
    "# Print results rounded to three decimal places\n",
    "print(\"Fake Data Mean Correlation Matrix:\")\n",
    "print(np.round(fake_mean_correlation, 3))\n",
    "\n",
    "print(\"\\nReal Data Mean Correlation Matrix:\")\n",
    "print(np.round(real_mean_correlation, 3))\n",
    "\n",
    "def plot_correlation_heatmap(correlation_matrix, title, feature_names):\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r', linewidths=.5, ax=ax, fmt=\".2f\", annot_kws={\"size\": 12}, vmin=-0.1, vmax=1)\n",
    "    \n",
    "    # 축 이름 설정\n",
    "    ax.set_xticklabels(feature_names, fontsize=12)\n",
    "    ax.set_yticklabels(feature_names, fontsize=12)\n",
    "    \n",
    "    # 제목 및 시각적 요소\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "feature_names = ['DJI', 'IXIC', 'JPM', 'HSI', 'Gold', 'WTI']\n",
    "plot_correlation_heatmap(real_mean_correlation, \"Real Data\", feature_names)\n",
    "plot_correlation_heatmap(fake_mean_correlation, \"Fake Data\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 누적 로그 리턴 플롯 \"\"\"\n",
    "# def compute_cumulative_log_returns(data):\n",
    "#     return np.cumsum(data, axis=2)  # Cumulative sum along the time axis\n",
    "\n",
    "# # 누적 로그 리턴 계산\n",
    "# real_cumulative_log_returns = compute_cumulative_log_returns(real_data)  # Shape: (batch_size, num_assets, sequence_length)\n",
    "# fake_cumulative_log_returns = compute_cumulative_log_returns(fake_data)  # Shape: (batch_size, num_assets, sequence_length)\n",
    "\n",
    "# # 각 자산별 플롯 생성\n",
    "# num_assets = real_data.shape[1]\n",
    "# for asset_idx in range(num_assets):\n",
    "#     # Real 데이터 플롯\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(real_cumulative_log_returns.shape[0]):  # 최대 10개의 샘플만 플롯\n",
    "#         plt.plot(real_cumulative_log_returns[i, asset_idx, :], alpha=0.6)\n",
    "#     plt.title(f'Real Cumulative Log Returns for Asset {asset_idx + 1}')\n",
    "#     plt.xlabel('Time Step')\n",
    "#     plt.ylabel('Cumulative Log Return')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Fake 데이터 플롯\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(fake_cumulative_log_returns.shape[0]):  # 최대 10개의 샘플만 플롯\n",
    "#         plt.plot(fake_cumulative_log_returns[i, asset_idx, :], alpha=0.6)\n",
    "#     plt.title(f'Fake Cumulative Log Returns for Asset {asset_idx + 1}')\n",
    "#     plt.xlabel('Time Step')\n",
    "#     plt.ylabel('Cumulative Log Return')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the performance of our model by first generating the price process, apply the prespecified trading strategies and compare the resulting PnL process using the real and fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.strategies import log_return_to_price\n",
    "\n",
    "eval_size = real_data.shape[0]\n",
    "fake_data_torch = torch.from_numpy(fake_data).float().transpose(1, 2)\n",
    "real_data_torch = torch.from_numpy(real_data).float().transpose(1, 2)\n",
    "\n",
    "fake_prices = log_return_to_price(fake_data_torch[:eval_size], test_init_price[:eval_size])\n",
    "real_prices = log_return_to_price(real_data_torch[:eval_size], test_init_price[:eval_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = 'src/evaluation/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    eval_config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "    \n",
    "print(fake_data_torch.shape, real_data_torch.shape, test_init_price.shape)\n",
    "\n",
    "all_positive = (fake_prices > 0).all()\n",
    "if not all_positive:\n",
    "    raise ValueError(\"Sanity Check Failed: Some fake prices are not positive.\")\n",
    "\n",
    "res_dict = {\"var_mean\" : 0., \"es_mean\": 0., \"max_drawback_mean\": 0., \"cumulative_pnl_mean\": 0.,}\n",
    "\n",
    "# Do final evaluation\n",
    "num_strat = 4\n",
    "\n",
    "with torch.no_grad():\n",
    "    for strat_name in ['equal_weight', 'mean_reversion', 'trend_following', 'vol_trading']:\n",
    "        subres_dict = full_evaluation(fake_prices, real_prices, eval_config, strat_name = strat_name)\n",
    "        filtered_means = {k: round(v, 4) for k, v in subres_dict.items() if '_mean' in k}\n",
    "        print(strat_name, filtered_means)\n",
    "        for k in res_dict:\n",
    "            res_dict[k] += subres_dict[k] / num_strat\n",
    "        \n",
    "for k, v in res_dict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare real and synthetic data for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from src.evaluations.plot import *\n",
    "from src.evaluations.evaluations import *\n",
    "val_set = TensorDataset(real_data_torch)\n",
    "fake_set = TensorDataset(fake_data_torch)\n",
    "\n",
    "fake_test_dl = DataLoader(\n",
    "    fake_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "real_test_dl = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "x_real, x_fake = loader_to_tensor(real_test_dl), loader_to_tensor(fake_test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(real_test_dl))[0].shape)\n",
    "print(next(iter(fake_test_dl))[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(real_test_dl, fake_test_dl, config, plot_show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylised facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hists_marginals(x_real, x_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above illustrate the marginal distribution comparisons across various time steps.\n",
    "Essentially we can quantify this by the marginal distribution loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marginal_loss = to_numpy(HistoLoss(x_real[:, 1:, :], n_bins=50, name='marginal_distribution')(x_fake[:, 1:, :]))\n",
    "print('Marginal Loss = ', Marginal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Our Model'\n",
    "compare_acf_matrix(real_test_dl, fake_test_dl, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots demonstrated the quality of the generated sample in terms of autocorrelation. Correspondingly, we have the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_loss = to_numpy(ACFLoss(x_real, name='auto_correlation')(x_fake))\n",
    "print('autocorrelation Loss = ', acf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossCorrelLoss 인스턴스 생성 시 name 인수를 추가\n",
    "cross_corr_loss = CrossCorrelLoss(x_real, max_lag=64, name=\"cross_correlation\")\n",
    "\n",
    "# 생성 데이터와 비교하여 손실 계산\n",
    "loss_value = cross_corr_loss.compute(x_fake)\n",
    "print('Cross-correlation Loss = ', loss_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CovLoss 인스턴스 생성\n",
    "cov_loss = CovLoss(x_real, name=\"covariance_loss\")\n",
    "\n",
    "# 생성 데이터와 비교하여 손실 계산\n",
    "loss_value = cov_loss.compute(x_fake)\n",
    "print('Covariance Loss = ', loss_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_prices_set = TensorDataset(fake_prices)\n",
    "real_prices_set = TensorDataset(real_prices)\n",
    "\n",
    "fake_prices_dl = DataLoader(\n",
    "    fake_prices_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "real_prices_dl = DataLoader(\n",
    "    real_prices_set,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tsne_plot(real_prices_dl, fake_prices_dl, config, plot_show =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
