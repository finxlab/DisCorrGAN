{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import pickle\n",
    "import ml_collections\n",
    "import wandb, signatory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from tqdm import tqdm\n",
    "sns.set_style(\"darkgrid\")  # 원하는 스타일 선택\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import *\n",
    "from src.evaluation.summary import full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import pickle\n",
    "import ml_collections\n",
    "import wandb, signatory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path as pt\n",
    "from tqdm import tqdm\n",
    "sns.set_style(\"darkgrid\")  # 원하는 스타일 선택\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.utils import *\n",
    "from src.evaluation.summary import full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration dict\n",
    "config_dir = 'configs/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "    \n",
    "torch.cuda.set_device(0)\n",
    "if (config.device == \"cuda\" and torch.cuda.is_available()):\n",
    "    config.update({\"device\": \"cuda:0\"}, allow_val_change=True)    \n",
    "else:\n",
    "    config.update({\"device\": \"cpu\"}, allow_val_change=True)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(data, window_size):\n",
    "    n_windows = data.shape[0] - window_size + 1\n",
    "    windows = np.zeros((n_windows, window_size, data.shape[1]))\n",
    "    for idx in range(n_windows):\n",
    "        windows[idx] = data[idx:idx + window_size]\n",
    "    return windows\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "df = pd.read_csv(\"./data/indices.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "df.set_index('Date', inplace=True)\n",
    "df = df.apply(pd.to_numeric).astype(float)\n",
    "#df.drop(columns='Hang Seng', inplace=True)\n",
    "\n",
    "\n",
    "# Step 2: Compute log 4\n",
    "log_returns = np.diff(np.log(df), axis=0)\n",
    "print(log_returns.shape)\n",
    "\n",
    "# Step 3: Scale the log returns\n",
    "log_returns_scaled, scalers = scaling(log_returns)\n",
    "\n",
    "# Step 4: Prepare initial prices and create rolling windows\n",
    "init_price = torch.from_numpy(np.array(df)[:-(config.n_steps), :]).float().unsqueeze(1)\n",
    "log_returns_scaled = torch.from_numpy(rolling_window(log_returns_scaled, config.n_steps)).float()\n",
    "log_returns_org = torch.from_numpy(rolling_window(log_returns, config.n_steps)).float()\n",
    "print('log_returns_scaled:', log_returns_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Return Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute cumulative log returns\n",
    "cumulative_log_returns = np.cumsum(log_returns, axis=0)\n",
    "\n",
    "# Step 4: Create a DataFrame for cumulative log returns\n",
    "cumulative_log_returns_df = pd.DataFrame(\n",
    "    cumulative_log_returns,\n",
    "    index=df.index[1:],  # Adjust index for cumulative log returns\n",
    "    columns=df.columns\n",
    ")\n",
    "\n",
    "# Step 5: Plot the cumulative log return paths\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in cumulative_log_returns_df.columns:\n",
    "    plt.plot(cumulative_log_returns_df.index, cumulative_log_returns_df[col], label=col)\n",
    "\n",
    "plt.title(\"Cumulative Log Return Paths of 6 Assets\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative Log Returns\", fontsize=14)\n",
    "plt.legend(title=\"Assets\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models for time series generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into training and validation set for the offline evaluation of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(log_returns_scaled.shape[0] * 0.7)\n",
    "\n",
    "training_data = log_returns_scaled[:train_size]\n",
    "test_data = log_returns_scaled[train_size:]\n",
    "\n",
    "train_init_price = init_price[:train_size]\n",
    "\n",
    "training_data_org = log_returns_org[:train_size]\n",
    "test_data_org = log_returns_org[train_size:]\n",
    "\n",
    "print(\"training_data: \", training_data.shape)\n",
    "print(\"test_data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = TensorDataset(training_data)\n",
    "test_set = TensorDataset(test_data)\n",
    "\n",
    "train_dl = DataLoader(training_set, batch_size=config.batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_set, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the generator, discriminator and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baselines.networks.discriminators import TCNDiscriminator\n",
    "from src.baselines.networks.generators import TCNGenerator\n",
    "from src.baselines.trainer import *\n",
    "\n",
    "generators = {}\n",
    "discriminators = {}\n",
    "\n",
    "for i in range(config.n_vars):\n",
    "    generators[i] = TCNGenerator(config).to(config.device)\n",
    "    discriminators[i] = TCNDiscriminator(config).to(config.device)\n",
    "trainer = GANTrainer(G=generators, D=discriminators, train_dl=train_dl, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model training\n",
    "#trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVFIT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, wasserstein_distance, ks_2samp, spearmanr, kendalltau\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "full_name = \"3_300_256_Glr_0.0002_Dlr_0.0001_hidden_dim_64_n_steps_256_corr_loss_l1_corr_weight_0.05_f_epoch_40_n_critic_2_gp_10.0_noise_4_Adam_drop_0.0_0.2_8_splitupdate_v2_corrlinear_overall_Adam0.5_withoutSA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Find the best epochs based on 100 days cumulative distribution \"\"\"\n",
    "\n",
    "from src.baselines.networks.generators import TCNGenerator\n",
    "#gen_tmp = Generator(config)\n",
    "min_dist = float('inf')  \n",
    "best_epoch = 0\n",
    "\n",
    "# rolling window 크기 (예: 100 days)\n",
    "window = 100\n",
    "\n",
    "def compute_avg_emd(real_data, fake_data, window):\n",
    "    \"\"\"\n",
    "    real_data, fake_data: 각각 shape (batch, 1, time)\n",
    "    각 창(window)마다의 합계를 구하고, 두 분포 간의 Earth Mover's Distance (EMD)를 구하여 합산합니다.\n",
    "    \"\"\"\n",
    "    emd = 0\n",
    "    # real_data.shape[1]가 1이더라도, loop로 general하게 처리 (만약 다중 feature인 경우 대비)\n",
    "    for i in range(real_data.shape[1]):\n",
    "        real_dist = rolling_window(real_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        fake_dist = rolling_window(fake_data[:, i, :].T, window).sum(axis=1).ravel()\n",
    "        emd += wasserstein_distance(real_dist, fake_dist)\n",
    "    return emd\n",
    "\n",
    "def load_sub_generator(file_path, config):\n",
    "    \"\"\"\n",
    "    TCNGenerator를 이용해 저장된 모델을 불러오고, device에 올린 후 eval 모드로 전환합니다.\n",
    "    \"\"\"\n",
    "    sub_gen = TCNGenerator(config)\n",
    "    sub_gen.load_state_dict(torch.load(file_path, map_location=config.device))\n",
    "    return sub_gen.to(config.device).eval()\n",
    "\n",
    "# 검사할 에폭 범위 (예: 70부터 90까지 2 에폭 간격으로 평가)\n",
    "#epochs_to_check = range(160, 220, 2)\n",
    "epochs_to_check = range(120, 122, 2)\n",
    "\n",
    "\n",
    "# 각 서브 생성자별 최소 EMD와 베스트 에폭 초기화\n",
    "min_emd = [float('inf')] * config.n_vars\n",
    "best_epoch = [None] * config.n_vars\n",
    "\n",
    "# 실제 데이터 로딩: training_data_org는 (batch, time, n_vars) 형태라고 가정하며,\n",
    "real_data = training_data_org.transpose(1, 2).cpu().numpy()\n",
    "\n",
    "\n",
    "# Generator 평가에 사용할 노이즈 생성 (예시: 3000 샘플)\n",
    "noise = torch.randn(1000, config.noise_dim, config.n_steps).to(config.device)\n",
    "\n",
    "# 각 에폭마다 각 서브 생성자(변수별)를 평가\n",
    "for epoch in epochs_to_check:\n",
    "    if epoch == 108:\n",
    "        continue\n",
    "    for i in range(config.n_vars):\n",
    "        # 저장된 모델의 경로 구성 (예전에 저장했던 경로와 파일명이 일치해야 합니다.)\n",
    "        file_path = os.path.join(f'./results/models/{full_name}/', f\"Generator_{epoch}_var_{i}.pt\")\n",
    "        # 파일이 존재하지 않으면 continue\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # 서브 생성자 로딩\n",
    "        sub_gen = load_sub_generator(file_path, config)        \n",
    "        with torch.no_grad():\n",
    "            fake = sub_gen(noise)  # fake의 shape는 (batch, 1, time) 이어야 합니다.\n",
    "        \n",
    "        # inverse_scaling_split 함수로 후처리 (scalers 및 변수 i에 맞게 진행)\n",
    "        fake_data = inverse_scaling_split(fake, scalers, i)  # scalers가 미리 정의되어 있어야 합니다.\n",
    "        \n",
    "        # 실제 데이터와 fake 데이터의 EMD 계산 (각 변수별로)\n",
    "        emd_val = compute_avg_emd(real_data[:, i:i+1, :], fake_data, window)\n",
    "        if emd_val < min_emd[i]:\n",
    "            min_emd[i] = emd_val\n",
    "            best_epoch[i] = epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch}: best_epoch = {best_epoch}, min_emd = {min_emd}\")\n",
    "\n",
    "print(\"\\n--- 최종 결과: 각 서브 생성자별 Best Epoch ---\")\n",
    "for i in range(config.n_vars):\n",
    "    print(f\"SubGen {i}: best epoch = {best_epoch[i]}, EMD = {min_emd[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = test_data_org.transpose(1, 2).cpu().numpy()\n",
    "best_epoch[0] = 178\n",
    "best_epoch[1] = 168\n",
    "best_epoch[2] = 144\n",
    "best_epoch[3] = 122\n",
    "best_epoch[4] = 172\n",
    "best_epoch[5] = 154\n",
    "\n",
    "fake_series_list = []\n",
    "for i in range(config.n_vars):    \n",
    "    best_ep = best_epoch[i]  # 각 서브 생성자의 가장 좋은 에폭  \n",
    "    file_path = os.path.join(f'./results/models/{full_name}/', f\"Generator_{best_ep}_var_{i}.pt\")\n",
    "    \n",
    "    # 서브 생성자 로딩: 수정된 load_sub_generator 함수 호출 (TCNGenerator 파라미터 제거)\n",
    "    sub_gen = load_sub_generator(file_path, config)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # fake의 shape: (batch, 1, n_steps)\n",
    "        fake = sub_gen(noise)\n",
    "    \n",
    "    # scalers[i]를 적용하여 inverse scaling 진행 (출력이 (batch, 1, n_steps) 형태임)\n",
    "    fake_data_i = inverse_scaling_split(fake, scalers, i)\n",
    "    # 필요 시 numpy array를 torch.Tensor로 변환 (device 및 dtype 맞춤)\n",
    "    fake_data_i = torch.tensor(fake_data_i, device=config.device)\n",
    "    \n",
    "    fake_series_list.append(fake_data_i)\n",
    "\n",
    "# 변수 축(dim=1)을 따라 결합: 최종 다변량 시계열의 shape는 (batch, n_vars, n_steps)\n",
    "fake_data = torch.cat(fake_series_list, dim=1)\n",
    "fake_data = fake_data.cpu().numpy()\n",
    "print(\"다변량 시계열 데이터의 shape:\", fake_data.shape)\n",
    "\n",
    "np.save('fake_data.npy', fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fake data와 Test data를 비교 \"\"\"\n",
    "\n",
    "for idx in range(6):\n",
    "    # 각 feature의 전체 최소값 및 최대값 계산\n",
    "    min_vals = np.expand_dims(real_data[:, idx, :], axis=1).min(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "    max_vals = np.expand_dims(real_data[:, idx, :], axis=1).max(axis=(0, 2), keepdims=True)  # Shape: (1, 5, 1)\n",
    "\n",
    "    # 마스크 계산 (모든 샘플이 범위 내에 있는지 확인)\n",
    "    mask_0 = np.all((np.expand_dims(fake_data[:, idx, :], axis=1) >= min_vals * 1.0) & (np.expand_dims(fake_data[:, idx, :], axis=1) <= max_vals * 1.0), axis=(1, 2))\n",
    "    mask_25 = np.all((np.expand_dims(fake_data[:, idx, :], axis=1) >= min_vals * 1.25) & (np.expand_dims(fake_data[:, idx, :], axis=1) <= max_vals * 1.25), axis=(1, 2))\n",
    "    mask_50 = np.all((np.expand_dims(fake_data[:, idx, :], axis=1) >= min_vals * 1.50) & (np.expand_dims(fake_data[:, idx, :], axis=1) <= max_vals * 1.50), axis=(1, 2))\n",
    "    mask_100 = np.all((np.expand_dims(fake_data[:, idx, :], axis=1) >= min_vals * 2.0) & (np.expand_dims(fake_data[:, idx, :], axis=1) <= max_vals * 2.0), axis=(1, 2))\n",
    "\n",
    "    # 마스크를 적용하여 fake_data 필터링\n",
    "    print(fake_data.shape, type(fake_data))\n",
    "\n",
    "    fake_data_0 = fake_data[mask_0]\n",
    "    fake_data_25 = fake_data[mask_25]\n",
    "    fake_data_50 = fake_data[mask_50]\n",
    "    fake_data_100 = fake_data[mask_100]\n",
    "    #fake_data = fake_data_50\n",
    "    print(fake_data_0.shape)\n",
    "    print(fake_data_25.shape)\n",
    "    print(fake_data_50.shape)\n",
    "    print(fake_data_100.shape)\n",
    "    print()\n",
    "    fake_data_tmp = fake_data[:3000]\n",
    "\n",
    "    print(real_data.shape)\n",
    "    print(f\"Filtered shape: {fake_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize the distribution of the real and fake data \"\"\"\n",
    "fake_list = [fake_data[:, i, :] for i in range(fake_data.shape[1])]\n",
    "real_list = [real_data[:, i, :] for i in range(real_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Plot the distribution of the real and fake data\n",
    "# windows = [1, 5, 20, 100]\n",
    "# for j in range(config.n_vars):\n",
    "#     fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(22, 4))  \n",
    "\n",
    "#     for i in range(len(windows)):\n",
    "#         col = i\n",
    "\n",
    "#         real_dist = rolling_window(real_list[j].T, windows[i]).sum(axis=1).ravel()\n",
    "#         fake_dist = rolling_window(fake_list[j].T, windows[i]).sum(axis=1).ravel()        \n",
    "        \n",
    "#         min_val = real_dist.min()\n",
    "#         max_val = real_dist.max()\n",
    "        \n",
    "#         bins = np.linspace(min_val, max_val, 81)  \n",
    "        \n",
    "#         sns.histplot(real_dist, bins=bins, kde=False, ax=axs[col], color='tab:blue', linewidth=0.1, alpha=0.5, stat='density')\n",
    "#         sns.histplot(fake_dist, bins=bins, kde=False, ax=axs[col], color='tab:orange', linewidth=0.1, alpha=0.5, stat='density')\n",
    "\n",
    "#         axs[col].set_xlim(*np.quantile(real_dist, [0.001, .999]))\n",
    "        \n",
    "#         axs[col].set_title('{} day return distribution'.format(windows[i]), size=15)\n",
    "#         axs[col].yaxis.grid(True, alpha=0.5)\n",
    "#         axs[col].set_xlabel('Cumulative log return', fontsize=12)\n",
    "#         axs[col].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "#     axs[0].legend(['Historical returns', 'Synthetic returns'])    \n",
    "#     #plt.rcParams['font.family'] = 'Times New Roman'\n",
    "#     plt.savefig(f\"figure_dist{j}.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# def calculate_distribution_scores(real, fake, num_G, windows):\n",
    "#     scores = {\n",
    "#         'EMD': np.zeros((num_G, len(windows))),\n",
    "#         'KL': np.zeros((num_G, len(windows))),\n",
    "#         'JS': np.zeros((num_G, len(windows))),\n",
    "#         'KS': np.zeros((num_G, len(windows)))\n",
    "#     }\n",
    "\n",
    "#     for i in range(num_G):\n",
    "#         for j in range(len(windows)):\n",
    "#             real_dist = rolling_window(real[i].T, windows[j]).sum(axis=1).ravel()\n",
    "#             fake_dist = rolling_window(fake[i].T, windows[j]).sum(axis=1).ravel()\n",
    "            \n",
    "#             np.random.shuffle(real_dist)\n",
    "#             np.random.shuffle(fake_dist)\n",
    "            \n",
    "#             # Calculate EMD\n",
    "#             scores['EMD'][i, j] = wasserstein_distance(real_dist, fake_dist)\n",
    "            \n",
    "#             # Calculate KS Statistic\n",
    "#             scores['KS'][i, j], _ = ks_2samp(real_dist, fake_dist)                                    \n",
    "            \n",
    "#             # Create histograms to estimate the probability distributions\n",
    "#             real_hist, bin_edges = np.histogram(real_dist, bins=100, density=True)\n",
    "#             fake_hist, _ = np.histogram(fake_dist, bins=bin_edges, density=True)\n",
    "            \n",
    "#             # Normalize the histograms to get probability distributions\n",
    "#             real_prob = real_hist / np.sum(real_hist)\n",
    "#             fake_prob = fake_hist / np.sum(fake_hist)\n",
    "            \n",
    "#             # Calculate KL Divergence\n",
    "#             kl_divergence = entropy(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "#             scores['KL'][i, j] = kl_divergence\n",
    "\n",
    "#             # Calculate JS Divergence\n",
    "#             js_divergence = jensenshannon(real_prob + 1e-10, fake_prob + 1e-10)\n",
    "#             scores['JS'][i, j] = js_divergence\n",
    "                \n",
    "#     df_scores = {}\n",
    "#     for metric, data in scores.items():\n",
    "#         data = np.round(data, decimals=4)\n",
    "#         df_scores[metric] = pd.DataFrame(data.T, index=windows, columns=[f'{metric} {i}' for i in range(num_G)])\n",
    "        \n",
    "#     emd_avg = np.mean(scores['EMD'], axis=0)\n",
    "#     df_scores['EMD']['EMD_avg'] = np.round(emd_avg, decimals=4)\n",
    "    \n",
    "#     return df_scores\n",
    "\n",
    "# # Calculate the distribution scores\n",
    "# windows = pd.Series([1, 5, 20, 100], name='window size')\n",
    "\n",
    "# results_emd = calculate_distribution_scores(real_list, fake_list, config.n_vars, windows)\n",
    "# results_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import skew, kurtosis\n",
    "\n",
    "# def calculate_skew_kurtosis_per_sample_with_avg(real_list, fake_list, windows):\n",
    "#     \"\"\"\n",
    "#     real_list, fake_list: [ (n_samples, T)-ndarray, ... ] 길이가 자산 수\n",
    "#     windows: 비교할 윈도우 크기 리스트, e.g. [1,5,20,100]\n",
    "\n",
    "#     반환:\n",
    "#       df_skew, df_kurt: index=windows, columns=[Asset1…AssetN, Avg]\n",
    "#       각 셀은 | mean_kurtosis_real - mean_kurtosis_fake |\n",
    "#     \"\"\"\n",
    "#     num_assets = len(real_list)\n",
    "#     asset_cols = [f\"Asset{i+1}\" for i in range(num_assets)]\n",
    "\n",
    "#     # 결과를 채울 빈 DataFrame 준비\n",
    "#     df_skew = pd.DataFrame(index=windows, columns=asset_cols, dtype=float)\n",
    "#     df_kurt = pd.DataFrame(index=windows, columns=asset_cols, dtype=float)\n",
    "\n",
    "#     for w in windows:\n",
    "#         for i in range(num_assets):\n",
    "#             real_arr = real_list[i]  # shape (n_samples, T)\n",
    "#             fake_arr = fake_list[i]\n",
    "            \n",
    "#             real_skews, real_kurts = [], []\n",
    "#             fake_skews, fake_kurts = [], []\n",
    "\n",
    "#             for k in range(real_arr.shape[0]):  # 샘플 단위 루프\n",
    "#                 # 각 샘플의 1D 시계열을 (T,1) 형태로 rolling_window에 넣기\n",
    "#                 real_win = rolling_window(real_arr[k:k+1].T, w).sum(axis=1).ravel()\n",
    "#                 fake_win = rolling_window(fake_arr[k:k+1].T, w).sum(axis=1).ravel()\n",
    "                \n",
    "#                 real_skews.append( skew(real_win) )\n",
    "#                 real_kurts.append( kurtosis(real_win, fisher=False) )\n",
    "#                 fake_skews.append( skew(fake_win) )\n",
    "#                 fake_kurts.append( kurtosis(fake_win, fisher=False) )\n",
    "\n",
    "#             # 샘플별 평균 왜도·첨도\n",
    "#             avg_real_sk = np.mean(real_skews)\n",
    "#             avg_fake_sk = np.mean(fake_skews)\n",
    "#             avg_real_ku = np.mean(real_kurts)\n",
    "#             avg_fake_ku = np.mean(fake_kurts)\n",
    "\n",
    "#             # 절대 차이 저장\n",
    "#             df_skew.at[w,  asset_cols[i]] = abs(avg_real_sk - avg_fake_sk)\n",
    "#             df_kurt.at[w, asset_cols[i]] = abs(avg_real_ku - avg_fake_ku)\n",
    "\n",
    "#         # 윈도우별(행별) 자산 평균 추가\n",
    "#         df_skew.at[w,  \"Avg\"] = df_skew.loc[w,  asset_cols].mean()\n",
    "#         df_kurt.at[w, \"Avg\"] = df_kurt.loc[w, asset_cols].mean()\n",
    "\n",
    "#     # 소수점 4자리로 정리\n",
    "#     return df_skew.round(4), df_kurt.round(4)\n",
    "\n",
    "# # --- 사용 예시 ---\n",
    "# windows = [1, 5, 20, 100]\n",
    "# df_skew_diff, df_kurt_diff = calculate_skew_kurtosis_per_sample_with_avg(\n",
    "#     real_list, fake_list, windows\n",
    "# )\n",
    "\n",
    "# print(\"윈도우별 Skewness Difference:\")\n",
    "# print(df_skew_diff)\n",
    "\n",
    "# print(\"\\n윈도우별 Kurtosis Difference:\")\n",
    "# print(df_kurt_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" visualizing acf plots \"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def plot_acf_comparison_3cols(real_list, fake_list, n_vars, lags=50):\n",
    "    \"\"\"\n",
    "    가로(1행)에 3개 컬럼:\n",
    "      1) Identity\n",
    "      2) Absolute\n",
    "      3) Squared\n",
    "    에 대해 ACF를 시각화하는 버전.\n",
    "    \n",
    "    real_list[i], fake_list[i]는 각각 i번째 자산(또는 그룹)에 대한 다수의 시계열 샘플을 갖고 있다고 가정.\n",
    "    \"\"\"\n",
    "    # 변환 함수 정의 (3개)\n",
    "    data_transforms = [\n",
    "        (lambda x: x,       'Identity', 'Identity log returns'),\n",
    "        (np.abs,            'Absolute', 'Absolute log returns'),\n",
    "        (np.square,         'Squared',  'Squared log returns')\n",
    "    ]\n",
    "    \n",
    "    for i in range(n_vars):\n",
    "        # 한 그룹(자산)에 대해 1행 x 3열 서브플롯 생성\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "        \n",
    "        # real_list[i], fake_list[i]는 각각 shape=(n_samples, length) 라고 가정\n",
    "        # 변환 후 각 시계열에 대해 ACF를 계산하고, 평균/표준편차를 시각화\n",
    "        \n",
    "        for ax, (transform_func, data_type, title) in zip(axs, data_transforms):\n",
    "            # (1) 변환 적용\n",
    "            transformed_real = transform_func(real_list[i])\n",
    "            transformed_fake = transform_func(fake_list[i])\n",
    "            \n",
    "            # (2) 각 시계열에 대해 ACF 계산\n",
    "            acf_real = np.array([acf(ts, nlags=lags) for ts in transformed_real])\n",
    "            acf_fake = np.array([acf(ts, nlags=lags) for ts in transformed_fake])\n",
    "            \n",
    "            # (3) 평균 ACF 및 표준편차\n",
    "            mean_real = acf_real.mean(axis=0)\n",
    "            std_real  = acf_real.std(axis=0)\n",
    "            mean_fake = acf_fake.mean(axis=0)\n",
    "            std_fake  = acf_fake.std(axis=0)\n",
    "            \n",
    "            # (4) 그래프 그리기\n",
    "            ax.plot(mean_real, label=f'{data_type} Real (mean)', color='tab:blue')\n",
    "            ax.fill_between(range(lags+1),\n",
    "                            mean_real - 0.5*std_real,\n",
    "                            mean_real + 0.5*std_real,\n",
    "                            color='tab:blue', alpha=0.2,\n",
    "                            label='Real ± 1/2 std')\n",
    "            \n",
    "            ax.plot(mean_fake, label=f'{data_type} Synthetic (mean)', color='tab:orange')\n",
    "            ax.fill_between(range(lags+1),\n",
    "                            mean_fake - 0.5*std_fake,\n",
    "                            mean_fake + 0.5*std_fake,\n",
    "                            color='tab:orange', alpha=0.2,\n",
    "                            label='Fake ± 1/2 std')\n",
    "            \n",
    "            ax.set_ylim(-0.15, 0.3)\n",
    "            ax.set_title(title, fontsize=13)\n",
    "            ax.grid(True)\n",
    "            ax.axhline(y=0, color='k', linewidth=0.8)\n",
    "            ax.axvline(x=0, color='k', linewidth=0.8)\n",
    "            ax.set_xlabel('Lag', fontsize=11)\n",
    "        \n",
    "        # 좌측 서브플롯에 범례를 표시 (첫 번째 col)\n",
    "        axs[0].legend()\n",
    "        \n",
    "        # 필요 시 레이아웃 조정\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"figure_acf{i}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "plot_acf_comparison_3cols(real_list, fake_list, n_vars=config.n_vars, lags=120)\n",
    "#plot_acf_comparison(real_list, fake_list, config.n_vars, lags=100)\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def calculate_acf_score_mean_vs_mean_no_std(\n",
    "    real_list,\n",
    "    fake_list,\n",
    "    lags=30,\n",
    "    loss_type='mse'\n",
    "):\n",
    "    \"\"\"\n",
    "    real_list[i], fake_list[i] 각각 하나의 \"그룹\" 또는 \"자산\" 집합이라고 가정\n",
    "    각 real_list[i]에는 (n_real_samples_i, ) 형태의 시계열이 여러 개 들어있다고 보고,\n",
    "    fake_list[i]에도 여러 시계열이 들어있다고 봄.\n",
    "\n",
    "    -> Real 평균 ACF vs. Fake 평균 ACF만을 비교하여 MSE, MAE 등을 계산하는 버전.\n",
    "       표준편차( std )는 따로 계산하지 않음.\n",
    "    \"\"\"\n",
    "    # 변환 함수 정의\n",
    "    data_transforms = [lambda x: x, np.abs, np.square]  \n",
    "    titles = ['Identity log returns', 'Absolute log returns', 'Squared log returns']\n",
    "\n",
    "    acf_scores = {}  # 그룹별 결과 저장\n",
    "    # 최종적으로 transform별 \"평균 점수(mean of means)\"를 계산할 목적\n",
    "    avg_scores_per_transform = {title: [] for title in titles}\n",
    "\n",
    "    n_vars = len(real_list)\n",
    "    for group_idx in range(n_vars):\n",
    "        group_scores = {}  # 이 그룹에 대한 transform별 점수(스칼라값)\n",
    "\n",
    "        for transform, title in zip(data_transforms, titles):\n",
    "            # (1) Real / Fake 데이터 변환\n",
    "            transformed_real = transform(real_list[group_idx])\n",
    "            transformed_fake = transform(fake_list[group_idx])\n",
    "            \n",
    "            # (2) 각 샘플별 ACF 계산\n",
    "            acf_real = np.array([acf(ts, nlags=lags) for ts in transformed_real])  \n",
    "            acf_fake = np.array([acf(ts, nlags=lags) for ts in transformed_fake])  \n",
    "\n",
    "            # (3) Real, Fake 각각의 평균 ACF 계산\n",
    "            mean_acf_real = np.mean(acf_real, axis=0)  # shape=(lags+1,)\n",
    "            mean_acf_fake = np.mean(acf_fake, axis=0)  # shape=(lags+1,)\n",
    "\n",
    "            # (4) Real 평균 ACF vs. Fake 평균 ACF 차이\n",
    "            diff = mean_acf_fake - mean_acf_real\n",
    "\n",
    "            # (5) loss_type에 따라 계산 (MSE or MAE 등)\n",
    "            if loss_type == 'mse':\n",
    "                val = np.mean(diff**2)\n",
    "            elif loss_type == 'mae':\n",
    "                val = np.mean(np.abs(diff))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
    "\n",
    "            # (6) 표준편차는 사용하지 않으므로, 결과는 스칼라 val만\n",
    "            group_scores[title] = round(val, 4)\n",
    "\n",
    "            # (7) 전체 평균 계산을 위해 저장\n",
    "            avg_scores_per_transform[title].append(val)\n",
    "\n",
    "        # 해당 그룹에 대한 transform별 점수 기록\n",
    "        acf_scores[f\"Group {group_idx+1}\"] = group_scores\n",
    "    \n",
    "    # 모든 그룹에 대한 “평균의 평균”(mean of means) 계산\n",
    "    overall_average_scores = {}\n",
    "    for title in titles:\n",
    "        mean_of_means = np.mean(avg_scores_per_transform[title])\n",
    "        overall_average_scores[title] = round(mean_of_means, 4)\n",
    "\n",
    "    return acf_scores, overall_average_scores\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "lags = 40\n",
    "loss_type = 'mse'  # 'mse', 'mae' 등\n",
    "\n",
    "acf_scores, overall_avg_scores = calculate_acf_score_mean_vs_mean_no_std(\n",
    "    real_list, \n",
    "    fake_list, \n",
    "    lags=lags, \n",
    "    loss_type=loss_type\n",
    ")\n",
    "\n",
    "print(\"=== ACF Scores (per group) ===\")\n",
    "for group_name, transform_scores in acf_scores.items():\n",
    "    print(f\"{group_name}:\")\n",
    "    for transform_title, val in transform_scores.items():\n",
    "        print(f\"  - {transform_title}: loss={val}\")\n",
    "\n",
    "print(\"\\n=== Overall Average Scores ===\")\n",
    "for transform_title, mean_of_means in overall_avg_scores.items():\n",
    "    print(f\"{transform_title}: mean of means={mean_of_means}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correlation_mean(data):\n",
    "#     \"\"\"\n",
    "#     각 샘플의 상관행렬을 계산한 후, 그 평균을 반환.\n",
    "    \n",
    "#     Parameters:\n",
    "#         data (np.ndarray): shape = (num_samples, num_time_steps, num_features)\n",
    "    \n",
    "#     Returns:\n",
    "#         mean_correlation_matrix (np.ndarray): shape = (num_features, num_features)\n",
    "#     \"\"\"\n",
    "#     num_samples = data.shape[0]\n",
    "#     correlations = []\n",
    "\n",
    "#     for i in range(num_samples):\n",
    "#         sample = data[i]  # shape = (num_time_steps, num_features)\n",
    "#         corr_mat = np.corrcoef(sample, rowvar=False)\n",
    "#         correlations.append(corr_mat)\n",
    "\n",
    "#     mean_correlation_matrix = np.mean(correlations, axis=0)  \n",
    "#     return mean_correlation_matrix\n",
    "\n",
    "\n",
    "# def correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mse'):\n",
    "#     \"\"\"\n",
    "#     Real과 Fake의 평균 상관행렬 간의 손실을 계산.\n",
    "    \n",
    "#     Parameters:\n",
    "#         real_mean_corr (np.ndarray): Real 데이터의 평균 상관행렬\n",
    "#         fake_mean_corr (np.ndarray): Fake 데이터의 평균 상관행렬\n",
    "#         loss_type (str): 'mse', 'mae', 'frobenius' 중 선택\n",
    "\n",
    "#     Returns:\n",
    "#         loss (float): 지정된 손실 타입에 따른 손실 값 (소수점 4자리로 반올림)\n",
    "#     \"\"\"\n",
    "#     diff = fake_mean_corr - real_mean_corr\n",
    "\n",
    "#     if loss_type == 'mse':\n",
    "#         loss_val = np.mean(diff**2)\n",
    "#     elif loss_type == 'mae':\n",
    "#         loss_val = np.mean(np.abs(diff))\n",
    "#     elif loss_type == 'frobenius':            \n",
    "#         loss_val = np.linalg.norm(diff, ord='fro')\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "#     return round(loss_val, 4)\n",
    "\n",
    "# # 상관행렬 계산을 위해 데이터 전치 (shape: (num_samples, num_time_steps, num_features))\n",
    "# real_mean_corr = correlation_mean(np.transpose(real_data, (0, 2, 1)))\n",
    "# fake_mean_corr = correlation_mean(np.transpose(fake_data, (0, 2, 1)))\n",
    "\n",
    "# # 손실 계산\n",
    "# loss_mae = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mae')\n",
    "# loss_frob = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='frobenius')\n",
    "# loss_mse = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mse')\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"[Correlation Loss - MAE]\")\n",
    "# print(f\"Loss: {loss_mae}\")\n",
    "\n",
    "# print(\"\\n[Correlation Loss - Frobenius]\")\n",
    "# print(f\"Loss: {loss_frob}\")\n",
    "\n",
    "# print(\"\\n[Correlation Loss - MSE]\")\n",
    "# print(f\"Loss: {loss_mse}\")\n",
    "\n",
    "\n",
    "# # def plot_correlation_heatmap(correlation_matrix, title, feature_names):\n",
    "# #     fig, ax = plt.subplots(figsize=(5, 4))\n",
    "# #     sns.heatmap(correlation_matrix, annot=True, cmap='cubehelix_r', linewidths=.5, ax=ax, fmt=\".2f\", annot_kws={\"size\": 12}, vmin=-0.1, vmax=1)\n",
    "    \n",
    "# #     # 축 이름 설정\n",
    "# #     ax.set_xticklabels(feature_names, fontsize=12)\n",
    "# #     ax.set_yticklabels(feature_names, fontsize=12)\n",
    "    \n",
    "# #     # 제목 및 시각적 요소\n",
    "# #     ax.set_title(title, fontsize=15)\n",
    "# #     ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# #     plt.tight_layout()\n",
    "# #     plt.show()\n",
    "\n",
    "# # feature_names = ['DJI', 'IXIC', 'JPM', 'HSI', 'Gold', 'WTI']\n",
    "# # plot_correlation_heatmap(real_mean_corr, \"Real Data\", feature_names)\n",
    "# # plot_correlation_heatmap(fake_mean_corr, \"Fake Data\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# def plot_correlation_heatmap(corr_mat, loss):\n",
    "#     fig, ax = plt.subplots(figsize=(3.5,3))\n",
    "\n",
    "#     sns.heatmap(\n",
    "#         np.abs(corr_mat),\n",
    "#         cmap='rocket',\n",
    "#         vmin=0, vmax=1,\n",
    "#         linewidths=0.01,\n",
    "#         linecolor=\"white\",\n",
    "#         ax=ax\n",
    "#     )\n",
    "\n",
    "#     # ticks & spines off\n",
    "#     ax.set_xticks([]); ax.set_yticks([])\n",
    "#     for s in ax.spines.values(): s.set_visible(False)\n",
    "\n",
    "#     # loss 숫자만 위쪽에\n",
    "#     ax.text(0.5, 1.02, f\"{loss:.3f}\",\n",
    "#             ha=\"center\", va=\"bottom\", transform=ax.transAxes,\n",
    "#             fontsize=14)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"fig_corr_real.png\", dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # 예시\n",
    "# real_loss = 0.000\n",
    "# fake_loss = correlation_loss(real_mean_corr, fake_mean_corr, loss_type='mae')\n",
    "\n",
    "# plot_correlation_heatmap(real_mean_corr, real_loss)\n",
    "# plot_correlation_heatmap(fake_mean_corr, fake_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "# def ccf_single_sample(x, max_lag=10):\n",
    "#     \"\"\"\n",
    "#     x: shape = (time_steps, features)\n",
    "#     - statsmodels.tsa.stattools.ccf(x[:, i], x[:, j])를 이용해\n",
    "#       i, j 모든 쌍에 대해 lag=0..max_lag 범위의 cross-correlation을 구한다.\n",
    "#     - 결과를 3D 배열 (features, features, max_lag+1)에 저장하여 반환.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     cc_matrix: np.ndarray\n",
    "#       shape = (num_features, num_features, max_lag+1)\n",
    "#       cc_matrix[i, j, k] = ccf 결과에서 lag=k인 상관계수\n",
    "#     \"\"\"\n",
    "#     time_steps, num_features = x.shape\n",
    "#     cc_matrix = np.zeros((num_features, num_features, max_lag+1), dtype=float)\n",
    "\n",
    "#     for i in range(num_features):\n",
    "#         for j in range(num_features):\n",
    "#             # statsmodels ccf: lag >= 0 에 대한 상관계수\n",
    "#             c_array = ccf(x[:, i], x[:, j], adjusted=False)\n",
    "#             # lag=0 ~ max_lag만 추출\n",
    "#             c_array = c_array[:(max_lag+1)]\n",
    "#             # ccf 함수는 무한대로 확장될 수 있으므로, 필요한 max_lag+1까지만 자르기\n",
    "#             if len(c_array) < (max_lag+1):\n",
    "#                 # 부족한 경우 0으로 채우기\n",
    "#                 c_array = np.pad(c_array, (0, max_lag+1 - len(c_array)), 'constant')\n",
    "#             cc_matrix[i, j, :] = c_array\n",
    "\n",
    "#     return cc_matrix\n",
    "\n",
    "# def ccf_mean(data, max_lag=20):\n",
    "#     \"\"\"\n",
    "#     data: shape = (num_samples, time_steps, features)\n",
    "\n",
    "#     각 샘플마다 ccf_single_sample -> (features, features, max_lag+1)\n",
    "#     모든 샘플에 대해 평균낸 결과를 반환.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     mean_ccf: np.ndarray\n",
    "#       shape = (num_features, num_features, max_lag+1)\n",
    "#     \"\"\"\n",
    "#     num_samples, time_steps, num_features = data.shape\n",
    "\n",
    "#     # 모든 샘플의 CCF 결과를 쌓아두기\n",
    "#     all_cc = np.zeros((num_samples, num_features, num_features, max_lag+1), dtype=float)\n",
    "\n",
    "#     for s_idx in range(num_samples):\n",
    "#         cc_matrix = ccf_single_sample(data[s_idx], max_lag=max_lag)\n",
    "#         all_cc[s_idx] = cc_matrix  # shape=(features, features, max_lag+1)\n",
    "\n",
    "#     # 샘플 차원에 대해 평균\n",
    "#     mean_ccf = all_cc.mean(axis=0)  # shape=(features, features, max_lag+1)\n",
    "#     return mean_ccf\n",
    "\n",
    "# def ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mse'):\n",
    "#     \"\"\"\n",
    "#     - real_mean_ccf: shape = (features, features, max_lag+1)\n",
    "#       Real 데이터의 평균 CCF 행렬\n",
    "#     - fake_mean_ccf: shape = (features, features, max_lag+1)\n",
    "#       Fake 데이터의 평균 CCF 행렬\n",
    "#     - loss_type: 'mse', 'mae', 'frobenius' 중 선택\n",
    "\n",
    "#     returns: loss (float), 소수점 4자리로 반올림\n",
    "#     \"\"\"\n",
    "#     # 두 평균 CCF 행렬의 차이\n",
    "#     diff = fake_mean_ccf - real_mean_ccf\n",
    "\n",
    "#     # 손실 계산\n",
    "#     if loss_type == 'mse':\n",
    "#         loss_val = np.mean(diff**2)\n",
    "#     elif loss_type == 'mae':\n",
    "#         loss_val = np.mean(np.abs(diff))\n",
    "#     elif loss_type == 'frobenius':\n",
    "#         # 3D 배열에 대한 Frobenius norm\n",
    "#         loss_val = np.linalg.norm(diff)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "#     return round(loss_val, 4)\n",
    "\n",
    "# # 사용 예시\n",
    "# max_lag = 10\n",
    "\n",
    "# # CCF 계산을 위해 데이터 전치 (shape: (num_samples, time_steps, num_features))\n",
    "# # 기존에 사용된 np.transpose(real_data, (0, 2, 1))은 이미 올바른 형상으로 가정\n",
    "# real_mean_ccf = ccf_mean(np.transpose(real_data, (0, 2, 1)), max_lag=max_lag)\n",
    "# fake_mean_ccf = ccf_mean(np.transpose(fake_data, (0, 2, 1)), max_lag=max_lag)\n",
    "\n",
    "# # 손실 계산\n",
    "# loss_mae = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mae')\n",
    "# loss_frob = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='frobenius')\n",
    "# loss_mse = ccf_loss_mean_vs_mean(real_mean_ccf, fake_mean_ccf, loss_type='mse')\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"[Partial CCF Loss - MAE]\")\n",
    "# print(f\"Loss: {loss_mae}\")\n",
    "\n",
    "# print(\"\\n[Partial CCF Loss - Frobenius]\")\n",
    "# print(f\"Loss: {loss_frob}\")\n",
    "\n",
    "# print(\"\\n[Partial CCF Loss - MSE]\")\n",
    "# print(f\"Loss: {loss_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def leverage_effect_loss_mean_vs_mean(real_list, fake_list, lags=30, loss_type='mse'):\n",
    "#     \"\"\"\n",
    "#     Real 데이터의 평균 레버리지 효과와 Fake 데이터의 평균 레버리지 효과를 비교하여 손실을 계산.\n",
    "\n",
    "#     Parameters:\n",
    "#         real_list (list of np.ndarray): 각 그룹별 Real 데이터. 각 요소는 shape=(n_real_samples, time_length)\n",
    "#         fake_list (list of np.ndarray): 각 그룹별 Fake 데이터. 각 요소는 shape=(n_fake_samples, time_length)\n",
    "#         lags (int): 최대 시차 (tau) 값\n",
    "#         loss_type (str): 손실 함수의 종류 ('mse', 'mae', 'fro')\n",
    "\n",
    "#     Returns:\n",
    "#         leverage_scores (dict): 그룹별 손실 값 저장\n",
    "#         overall_mean (float): 모든 그룹의 평균 손실\n",
    "#     \"\"\"\n",
    "\n",
    "#     def leverage_effect(ts, tau):\n",
    "#         \"\"\"\n",
    "#         레버리지 효과 계산:\n",
    "#         Corr(r_t, r_(t+tau)^2)\n",
    "#         \"\"\"\n",
    "#         rt = ts[:-tau]            # 뒤에서 tau 개 제외\n",
    "#         rt_squared = ts[tau:]**2  # 앞에서 tau 개 제외한 뒤 제곱\n",
    "#         return np.corrcoef(rt, rt_squared)[0, 1]  # 상관계수\n",
    "\n",
    "#     n_vars = len(real_list)\n",
    "#     leverage_scores = {}     # 그룹별 손실 값 저장\n",
    "#     all_group_losses = []    # 모든 그룹의 손실 값을 모아 전체 평균 손실 계산\n",
    "\n",
    "#     for i in range(n_vars):\n",
    "#         real_data = real_list[i]  # shape=(n_real_samples, time_length)\n",
    "#         fake_data = fake_list[i]  # shape=(n_fake_samples, time_length)\n",
    "\n",
    "#         # (1) Real 각 샘플의 레버리지 효과 (tau=1부터 lags까지)\n",
    "#         real_leverage_effects = np.array([\n",
    "#             [leverage_effect(ts, tau) for tau in range(1, lags+1)]\n",
    "#             for ts in real_data\n",
    "#         ])  # shape = (n_real_samples, lags)\n",
    "\n",
    "#         # (2) Fake 각 샘플의 레버리지 효과\n",
    "#         fake_leverage_effects = np.array([\n",
    "#             [leverage_effect(ts, tau) for tau in range(1, lags+1)]\n",
    "#             for ts in fake_data\n",
    "#         ])  # shape = (n_fake_samples, lags)\n",
    "\n",
    "#         # (3) Real의 평균 레버리지 효과 계산\n",
    "#         mean_real_leverage = np.mean(real_leverage_effects, axis=0)  # shape=(lags,)\n",
    "\n",
    "#         # (4) Fake의 평균 레버리지 효과 계산\n",
    "#         mean_fake_leverage = np.mean(fake_leverage_effects, axis=0)  # shape=(lags,)\n",
    "\n",
    "#         # (5) Real 평균과 Fake 평균 간의 차이 계산\n",
    "#         diff = mean_fake_leverage - mean_real_leverage\n",
    "\n",
    "#         # (6) loss_type에 따라 손실 계산\n",
    "#         if loss_type == 'mse':\n",
    "#             loss_val = np.mean(diff**2)\n",
    "#         elif loss_type == 'mae':\n",
    "#             loss_val = np.mean(np.abs(diff))\n",
    "#         elif loss_type == 'frobenius':\n",
    "#             loss_val = np.linalg.norm(diff)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
    "\n",
    "#         # (7) 손실 값 저장 (소수점 반올림은 최종 저장 시점에 적용)\n",
    "#         leverage_scores[f'Group {i+1}'] = loss_val\n",
    "#         all_group_losses.append(loss_val)\n",
    "\n",
    "#         # (8) 그룹별 손실 출력 (소수점 반올림)\n",
    "#         print(f\"[Group {i+1}] Leverage Loss ({loss_type.upper()}) => {loss_val:.4f}\")\n",
    "\n",
    "#     # (9) 전체 그룹의 평균 손실 계산\n",
    "#     overall_mean = np.mean(all_group_losses)\n",
    "#     print(f\"\\n[Overall] Leverage Loss ({loss_type.upper()}) => {overall_mean:.4f}\")\n",
    "\n",
    "#     # (10) 최종 결과를 소수점 4자리로 반올림하여 저장\n",
    "#     for group in leverage_scores:\n",
    "#         leverage_scores[group] = round(leverage_scores[group], 4)\n",
    "#     overall_mean = round(overall_mean, 4)\n",
    "\n",
    "#     return leverage_scores, overall_mean\n",
    "\n",
    "# # 예시 사용\n",
    "\n",
    "# # 손실 계산 (예: 'mse', 'mae', 'frobenius' 중 선택)\n",
    "# loss_type = 'mse'  # 'mse', 'mae', 'frobenius'\n",
    "# lags = 40\n",
    "# leverage_scores, overall_mean = leverage_effect_loss_mean_vs_mean(\n",
    "#     real_list, \n",
    "#     fake_list, \n",
    "#     lags=lags, \n",
    "#     loss_type=loss_type\n",
    "# )\n",
    "\n",
    "# # 최종 결과 출력\n",
    "# print(\"\\n=== Leverage Effect Loss (Mean vs. Mean) ===\")\n",
    "# for group, loss in leverage_scores.items():\n",
    "#     print(f\"{group}: {loss}\")\n",
    "# print(f\"Overall Mean Loss: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# fake_data: (10000, 1, 754)\n",
    "# 데이터를 squeeze하여 2D 배열로 변환\n",
    "idx = 4\n",
    "fake_data_new = fake_data[:, idx, :].squeeze()  # shape: (10000, 754)\n",
    "\n",
    "# 누적 로그 리턴 계산\n",
    "cumulative_returns = np.cumsum(fake_data_new[:100], axis=1)  # 첫 10개 샘플에 대해 누적합 계산\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, cum_return in enumerate(cumulative_returns):\n",
    "    plt.plot(cum_return, label=f'Sample {i+1}', alpha=0.8, linewidth=0.5)  # 각 샘플의 누적 로그 리턴을 그리기\n",
    "\n",
    "# 그래프 설정\n",
    "plt.title(\"Cumulative Log Returns of 500 Samples\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative Log Return\", fontsize=14)\n",
    "#plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.ylim(-0.5, 0.4)  # <-- 동일 범위로 맞춤\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# fake_data: (10000, 1, 754)\n",
    "# 데이터를 squeeze하여 2D 배열로 변환\n",
    "real_data_new = real_data[:, idx, :].squeeze()  # shape: (10000, 754)\n",
    "perm_idx = np.random.permutation(real_data_new.shape[0])[:1000]\n",
    "real_data_new = real_data_new[perm_idx]\n",
    "# 누적 로그 리턴 계산\n",
    "cumulative_returns = np.cumsum(real_data_new[:100], axis=1)  # 첫 10개 샘플에 대해 누적합 계산\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, cum_return in enumerate(cumulative_returns):\n",
    "    plt.plot(cum_return, label=f'Sample {i+1}', alpha=0.8, linewidth=0.5)  # 각 샘플의 누적 로그 리턴을 그리기\n",
    "\n",
    "# 그래프 설정\n",
    "plt.title(\"Cumulative Log Returns of 500 Samples\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Cumulative Log Return\", fontsize=14)\n",
    "#plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.5, 0.4)  # <-- 동일 범위로 맞춤\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" 누적 로그 리턴 플롯 \"\"\"\n",
    "# def compute_cumulative_log_returns(data):\n",
    "#     return np.cumsum(data, axis=2)  # Cumulative sum along the time axis\n",
    "\n",
    "# # 누적 로그 리턴 계산\n",
    "# real_cumulative_log_returns = compute_cumulative_log_returns(real_data)  # Shape: (batch_size, num_assets, sequence_length)\n",
    "# fake_cumulative_log_returns = compute_cumulative_log_returns(fake_data)  # Shape: (batch_size, num_assets, sequence_length)\n",
    "\n",
    "# # 각 자산별 플롯 생성\n",
    "# num_assets = real_data.shape[1]\n",
    "# for asset_idx in range(num_assets):\n",
    "#     # Real 데이터 플롯\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(real_cumulative_log_returns.shape[0]):  # 최대 10개의 샘플만 플롯\n",
    "#         plt.plot(real_cumulative_log_returns[i, asset_idx, :], alpha=0.6)\n",
    "#     plt.title(f'Real Cumulative Log Returns for Asset {asset_idx + 1}')\n",
    "#     plt.xlabel('Time Step')\n",
    "#     plt.ylabel('Cumulative Log Return')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Fake 데이터 플롯\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(fake_cumulative_log_returns.shape[0]):  # 최대 10개의 샘플만 플롯\n",
    "#         plt.plot(fake_cumulative_log_returns[i, asset_idx, :], alpha=0.6)\n",
    "#     plt.title(f'Fake Cumulative Log Returns for Asset {asset_idx + 1}')\n",
    "#     plt.xlabel('Time Step')\n",
    "#     plt.ylabel('Cumulative Log Return')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the performance of our model by first generating the price process, apply the prespecified trading strategies and compare the resulting PnL process using the real and fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.strategies import log_return_to_price\n",
    "\n",
    "eval_size = real_data.shape[0]\n",
    "#eval_size = 1500\n",
    "#real_data = test_data_org.transpose(1,2).cpu().numpy()\n",
    "fake_data_torch = torch.from_numpy(fake_data).float().transpose(1, 2)\n",
    "real_data_torch = torch.from_numpy(real_data).float().transpose(1, 2)\n",
    "\n",
    "test_init_price = train_init_price\n",
    "test_init_price[:] = 100\n",
    "\n",
    "fake_prices = log_return_to_price(fake_data_torch[:eval_size], test_init_price[:eval_size])\n",
    "real_prices = log_return_to_price(real_data_torch[:eval_size], test_init_price[:eval_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = 'src/evaluation/config.yaml'\n",
    "with open(config_dir) as file:\n",
    "    eval_config = ml_collections.ConfigDict(yaml.safe_load(file))\n",
    "\n",
    "print(fake_data_torch.shape, real_data_torch.shape, test_init_price.shape)\n",
    "\n",
    "all_positive = (fake_prices > 0).all()\n",
    "if not all_positive:\n",
    "    raise ValueError(\"Sanity Check Failed: Some fake prices are not positive.\")\n",
    "\n",
    "# 누적하기 위한 딕셔너리(평균 낼 것들이면 0.으로 초기화)\n",
    "res_dict = {\n",
    "    \"var_abs_mean\": 0.,\n",
    "    \"var_rel_mean\": 0.,\n",
    "    \"es_abs_mean\": 0.,\n",
    "    \"es_rel_mean\": 0.,\n",
    "}\n",
    "\n",
    "num_strat = 4  # 전략 개수\n",
    "\n",
    "with torch.no_grad():\n",
    "    for strat_name in ['equal_weight', 'mean_reversion', 'trend_following', 'vol_trading']:\n",
    "        # 각 전략에 대해 평가 함수 호출\n",
    "        subres_dict = full_evaluation(fake_prices, real_prices, eval_config, strat_name=strat_name)\n",
    "\n",
    "        # subres_dict에서 *_mean 항목만 골라서 소수점 4자리 반올림\n",
    "        filtered_means = {\n",
    "            k: round(v, 4) for k, v in subres_dict.items() if '_mean' in k\n",
    "        }\n",
    "        print(f\"{strat_name}: {filtered_means}\")\n",
    "\n",
    "        # res_dict에 누적\n",
    "        for k in res_dict:\n",
    "            # subres_dict가 해당 키를 갖고 있다고 가정\n",
    "            res_dict[k] += subres_dict[k] / num_strat\n",
    "\n",
    "# 최종적으로 각 키에 대해 평균화된 값 출력\n",
    "for k, v in res_dict.items():\n",
    "    print(k, round(v, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.evaluation.strategies import log_return_to_price\n",
    "\n",
    "# #eval_size = real_data.shape[0]\n",
    "# eval_size = 1600\n",
    "# real_data = test_data_org.transpose(1,2).cpu().numpy()\n",
    "# fake_data_torch = torch.from_numpy(fake_data).float().transpose(1, 2)\n",
    "# real_data_torch = torch.from_numpy(real_data).float().transpose(1, 2)\n",
    "\n",
    "# test_init_price = train_init_price\n",
    "# #test_init_price[:] = 100\n",
    "\n",
    "# fake_prices = log_return_to_price(fake_data_torch[:eval_size], test_init_price[:eval_size])\n",
    "# real_prices = log_return_to_price(real_data_torch[:eval_size], test_init_price[:eval_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.evaluations.plot import *\n",
    "# from src.evaluations.evaluations import *\n",
    "    \n",
    "# # 1) numpy array를 torch.Tensor로 변환\n",
    "# real_tensor = torch.from_numpy(real_data).float()[:1600, :, :24]   # torch.Size([1602, 6, 256])\n",
    "# fake_tensor = torch.from_numpy(fake_data).float()[:1600, :, :24]   # torch.Size([3000, 6, 256])\n",
    "\n",
    "# # 2) TensorDataset & DataLoader 생성\n",
    "# real_ds = TensorDataset(real_tensor)\n",
    "# real_test_dl = DataLoader(real_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "# fake_ds = TensorDataset(fake_tensor)\n",
    "# fake_test_dl = DataLoader(fake_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# tsne_plot(real_test_dl,fake_test_dl,config,plot_show =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative_score,_ = compute_discriminative_score(\n",
    "#             real_test_dl, real_test_dl, fake_test_dl, fake_test_dl, config, 10, 1, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_score_mean, _ = compute_predictive_score(\n",
    "#             real_train_dl, real_test_dl, fake_train_dl, fake_test_dl, config, 32, 2, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
